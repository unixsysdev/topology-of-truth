# Base configuration for GRU intervention experiment

# Model settings
base_model:
  name: "Qwen/Qwen3-0.6B"
  dtype: "bfloat16"
  device: "cuda"
  
teacher_model:
  name: "Qwen/Qwen3-1.7B"  # Better performing on our dataset
  dtype: "bfloat16"
  device: "cuda"

# Intervention settings
intervention:
  # Layer 4 = early intervention, gives model ~24 layers to use correction
  # For 28-layer model (0.6B): layer 4 is ~14% depth
  # Alternatively use ratio (e.g., 0.15) to auto-compute based on model depth
  layer: 4
  layer_ratio: null  # Set to 0.15 to use ratio instead of absolute layer
  layers_to_monitor: [2, 4, 6]  # For debugging/analysis  # Optional: multi-layer signal
  
# GRU Controller settings
controller:
  # Encoder
  encoder_input_dim: 256  # Projected from hidden_size
  encoder_hidden_dim: 256
  encoder_num_layers: 2
  encoder_dropout: 0.1
  
  # Latent space
  latent_dim: 64
  
  # Gate
  gate_hidden_dim: 128
  gate_threshold: 0.5  # Intervention threshold
  
  # Decoder (low-rank)
  decoder_rank: 16  # Low-rank decomposition
  decoder_hidden_dim: 256
  
# Local entropy proxy
entropy_proxy:
  method: "cosine_coherence"  # or "variance"
  window_size: 32  # Tokens to consider
  ema_decay: 0.9  # Exponential moving average for smoothing

# Training settings
training:
  # Phase 1: Pattern discovery
  phase1:
    epochs: 10
    batch_size: 4
    learning_rate: 1e-4
    clustering_method: "kmeans"
    n_clusters: 16  # Number of latent "sub-goal" patterns
    
  # Phase 2: Intervention training
  phase2:
    method: "hybrid"  # "supervised", "rl", or "hybrid"
    epochs: 20
    batch_size: 2
    learning_rate: 5e-5
    
    # Supervised component
    distillation_weight: 0.5
    
    # RL component
    entropy_reward_weight: 1.0
    accuracy_reward_weight: 10.0
    gate_sparsity_weight: 0.1  # Encourage sparse intervention
    
# Data settings
data:
  dataset: "openai/gsm8k"
  split: "train"
  max_samples: 500  # Start small
  max_seq_length: 2048
  
# Evaluation
evaluation:
  compute_tda: true
  tda_samples: 50  # Subset for TDA (expensive)
  metrics:
    - accuracy
    - h0_entropy
    - dust_score
    - gate_activation_rate

# Hardware optimization
optimization:
  gradient_checkpointing: true
  mixed_precision: true
  compile_model: false  # torch.compile - can be unstable
  
# Logging
logging:
  wandb_project: "topology-gru-intervention"
  log_every_n_steps: 10
  save_every_n_epochs: 5
